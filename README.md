# Syzygy-of-thoughts
ğŸš€ **Hello, [ç¤¾äº¤å¹³å°è´¦å·](https://www.youtube.com/watch?v=gZABle836yM)**  
- xxxxxxxxxxxxxxxxxxxxxxx

<div align='left'>
<img src="assets/rader.png" alt="teaser" width="400" />
</div>
æœ¬é¡¹ç›®ä¸ºè®ºæ–‡ã€ŠSyzygy of Thoughts: Enhancing LLM Reasoning with Minimal Free Resolutionã€‹çš„ä»£ç å®ç°ã€‚è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¨ç†æ¡†æ¶ Syzygy of Thoughts (SoT)ï¼Œé€šè¿‡å°†äº¤æ¢ä»£æ•°ä¸åŒè°ƒä»£æ•°ä¸­çš„ æœ€å°è‡ªç”±åˆ†è§£ (Minimal Free Resolution, MFR) åŸç†èå…¥æ€ç»´é“¾ (Chain of Thought, CoT)ï¼Œæ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚SoT é’ˆå¯¹ä¼ ç»Ÿ CoT åœ¨é«˜ç»´ã€éçº¿æ€§æ¨ç†ä¸­çš„å±€é™ï¼Œå¼•å…¥æ¨¡å—ã€è‡ªç”±æ€§ã€æ˜ å°„ã€ç²¾ç¡®æ€§ã€æœ€å°æ€§å’Œ Betti æ•°ç­‰ç»“æ„åŒ–åˆ†è§£ç­–ç•¥ï¼Œå°†å¤æ‚é—®é¢˜è½¬åŒ–ä¸ºç´§å‡‘ä¸”é€»è¾‘è‡ªæ´½çš„æ¨ç†å•å…ƒã€‚
å®éªŒåœ¨ GSM8K å’Œ MATH ç­‰æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œæ¶µç›– GPT-4o-miniã€Qwen2.5 ç­‰æ¨¡å‹ï¼ŒSoT çš„æ¨ç†å‡†ç¡®ç‡è¾¾åˆ°æˆ–è¶…è¶Šä¸»æµ CoT æ ‡å‡†ï¼ˆä¾‹å¦‚ï¼ŒGSM8K 96.0%ï¼ŒMATH 79.1%ï¼‰ï¼Œä¸”åœ¨é«˜æ¸©æ¡ä»¶ä¸‹ä¿æŒç¨³å®šæ€§ï¼Œæ¨ç†æ—¶é—´æ›´å…·å¯æ‰©å±•æ€§ã€‚


<div align='left'>
<img src="assets/main.png" alt="teaser" width="800" />
</div>






## Highlights
- *
- *


## Overview
- [Schedule](#schedule)
- [Citation](#citation)
- [Installation](#installation)
- [API Configuration Setup](#api-configuration-setup)
- [Data Preparation](#data-preparation)
- [Quick Start](#quick-start)
- [Model Zoo](#model-zoo)


## Schedule
ä»‹ç»xxxxxx:

- [x] Release model code of PTv3;
- [x] Release scratched config and record of indoor semantic segmentation;
  - [x] ScanNet
  - [x] ScanNet200
  - [x] S3DIS
  - [x] S3DIS 6-Fold (with cross-validation script) 


## Citation
If you find Syzygy-of-thoughts useful to your research, please cite our work as an acknowledgment. (*^â–½^*)
bib
@article{zhang2023faster,
  title={Faster segment anything: Towards lightweight sam for mobile applications},
  author={Zhang, Chaoning and Han, Dongshen and Qiao, Yu and Kim, Jung Uk and Bae, Sung-Ho and Lee, Seungkyu and Hong, Choong Seon},
  journal={arXiv preprint arXiv:2306.14289},
  year={2023}
}


## Installation
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

## API Configuration Setup

Before using the Atom of Thoughts (AoT) framework, you need to set up your API key and URL:

1. Create an apikey.py file in the project root directory with the following format:

url = "https://api.openai.com/v1"  # Replace with your API endpoint
api_key = [
    "your-api-key-here",  # Replace with your actual API key
    # You can add multiple API keys to improve concurrency performance.
]

### Api Key è·å–é€”å¾„




### Requirements
 Syzygy-of-thoughts :

(Recommendation)
- Ubuntu: 
- CUDA: 
- PyTorch: 

### Environment

- Base environment
bash
# æ”¹æˆæˆ‘ä»¬è‡ªå·±çš„
conda create -n pointcept python=3.8 -y
conda activate pointcept
conda install ninja -y
conda install pytorch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 pytorch-cuda=11.8 -c pytorch -c nvidia
conda install h5py pyyaml -c anaconda -y
conda install sharedarray tensorboard tensorboardx yapf addict einops scipy plyfile termcolor timm -c conda-forge -y
conda install pytorch-cluster pytorch-scatter pytorch-sparse -c pyg -y
pip install torch-geometric

cd libs/pointops
python setup.py install
cd ../..

# spconv (SparseUNet)
# refer https://github.com/traveller59/spconv
pip install spconv-cu118  # choose version match your local cuda version

# Open3D (visualization, optional)
pip install open3d



## Data Preparation
Please further refer xxx

## Quick Start
xxxxxxxxxxxxx


## Model Zoo
### 1. å®éªŒæ€§èƒ½
ä»¥ä¸‹è¡¨æ ¼æ¯”è¾ƒäº† CoTã€CoT-SC (n=5) å’Œ SoT åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œæ¶µç›–æ•°å­¦æ¨ç†ï¼ˆGSM8Kã€SVAMPã€MultiArithã€ASDivã€AQUAï¼‰ã€é€šç”¨çŸ¥è¯†ï¼ˆMMLUï¼‰ã€å¤šä»»åŠ¡é—®ç­”ï¼ˆBBHï¼‰ã€æ—¶é—´æ¨ç†ï¼ˆDateï¼‰å’Œé€»è¾‘æ¨ç†ï¼ˆCLUTRRï¼‰ã€‚SoT åœ¨æ‰€æœ‰æ¨¡å‹å’Œä»»åŠ¡ä¸­å‡å–å¾—æœ€ä½³è¡¨ç°ã€‚
| **Method**                        | **GSM8K** | **SVAMP** | **MultiArith** | **ASDiv** | **AQUA** | **MMLU** | **BBH** | **Date** | **CLUTRR** |
|------------------------------------|:---------:|:---------:|:--------------:|:---------:|:--------:|:--------:|:-------:|:--------:|:----------:|
| **GPT-4o-mini**                    |           |           |                |           |          |          |         |          |            |
| CoT                                | 85.1%     | 84.4%     | 99.2%          | 97.0%     | 65.0%    | 63.1%    | 66.3%   | 51.8%    | 65.9%      |
| CoT-SC (n=5)                       | 90.1%     | 86.0%     | 99.5%          | 98.5%     | 70.9%    | 67.3%    | 69.2%   | 54.9%    | 72.4%      |
| **SoT (Ours)**                     | **96.0%** | **92.2%** | **99.7%**      | **99.8%** | **75.6%** | **75.2%** | **72.8%** | **75.2%** | **75.7%**  |
| **Qwen2.5-Coder-7B-Instruct**      |           |           |                |           |          |          |         |          |            |
| CoT                                | 77.2%     | 82.4%     | 92.3%          | 92.0%     | 60.6%    | 55.1%    | 47.1%   | 31.0%    | 20.1%      |
| CoT-SC (n=5)                       | 80.2%     | 84.1%     | 95.0%          | 95.0%     | 62.2%    | 56.3%    | 49.3%   | 32.9%    | 21.0%      |
| **SoT (Ours)**                     | **89.1%** | **90.6%** | **97.0%**      | **99.8%** | **63.3%** | **57.1%** | **57.3%** | **36.2%** | **26.3%**  |
| **Qwen2.5-VL-72B-Instruct**        |           |           |                |           |          |          |         |          |            |
| CoT                                | 86.1%     | 86.9%     | 98.8%          | 98.0%     | 81.1%    | 80.1%    | 77.3%   | 75.2%    | 70.1%      |
| CoT-SC (n=5)                       | 89.1%     | 88.2%     | 99.3%          | 98.4%     | 83.9%    | 82.9%    | 79.0%   | 78.0%    | 75.0%      |
| **SoT (Ours)**                     | **96.0%** | **95.8%** | **99.7%**      | **99.2%** | **89.4%** | **84.3%** | **85.3%** | **80.2%** | **78.9%**  |
| **Gemma-3-27b-it**                 |           |           |                |           |          |          |         |          |            |
| CoT                                | 83.1%     | 85.9%     | 91.9%          | 98.5%     | 80.3%    | 70.8%    | 70.7%   | 76.9%    | 65.3%      |
| CoT-SC (n=5)                       | 87.1%     | 87.0%     | 92.3%          | 99.2%     | 85.4%    | 73.2%    | 73.2%   | 80.2%    | 66.4%      |
| **SoT (Ours)**                     | **96.0%** | **95.8%** | **99.7%**      | **99.2%** | **89.4%** | **84.3%** | **85.3%** | **80.2%** | **78.9%**  |
| **Gemma-3-12b-it**                 |           |           |                |           |          |          |         |          |            |
| CoT                                | 83.2%     | 79.0%     | 90.4%          | 97.7%     | 68.9%    | 68.1%    | 64.6%   | 77.7%    | 49.0%      |
| CoT-SC (n=5)                       | 86.1%     | 81.0%     | 93.3%          | 98.0%     | 71.7%    | 70.6%    | 66.7%   | 80.2%    | 52.2%      |
| **SoT (Ours)**                     | **92.1%** | **92.5%** | **96.1%**      | **99.2%** | **77.2%** | **72.3%** | **69.1%** | **82.5%** | **55.0%**  |
